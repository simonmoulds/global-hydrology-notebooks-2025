{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Exploring Precipitation Patterns in UK Catchments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ## Objectives\n",
    "\n",
    "\n",
    "\n",
    "  By the end of this tutorial, you will:\n",
    "\n",
    "\n",
    "\n",
    "  - Understand and calculate key precipitation indices\n",
    "\n",
    "\n",
    "\n",
    "  - Visualize precipitation data using various techniques\n",
    "\n",
    "\n",
    "\n",
    "  - Reinforce Python skills in data analysis and visualization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ## Prerequisites:\n",
    "\n",
    "\n",
    "\n",
    "  - Basic Python understanding.\n",
    "\n",
    "\n",
    "\n",
    "  - Familiarity with pandas and matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Dataset\n",
    "\n",
    "\n",
    "\n",
    "  We will return to the CAMELS-GB dataset we used previously. Let's load data for one catchment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "# Use upper case for constants\n",
    "DATADIR = os.path.join('data', '8344e4f3-d2ea-44f5-8afa-86d2987543a9', 'data')\n",
    "\n",
    "# Load the data\n",
    "id = '97002'\n",
    "data = pd.read_csv(os.path.join(DATADIR, 'timeseries', f'CAMELS_GB_hydromet_timeseries_{id}_19701001-20150930.csv'), parse_dates=[0])\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It's a good idea to check how many NaN values we have in the precipitation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data['precipitation'].isnull().sum())\n",
    "# # Fill missing data or drop (depending on context)\n",
    "# data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Let's start by plotting a scatter plot of the data:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['date'], data['precipitation'], color='b', alpha=0.7)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Daily precipitation')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This is a lot of data to try to interpret. One thing that may interest us is the annual maximum daily precipitation. We can find this by grouping our dataframe by year, and retrieving the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amax = data.groupby(data['date'].dt.year)[['precipitation']].max().reset_index()\n",
    "amax.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(amax['date'], amax['precipitation'], color='b', alpha=0.7)\n",
    "# plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Annual maximum daily precipitation')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It looks as though there might be a trend. How can we assess this quantitatively? One commonly used approach is to use the Theil-Sen estimator. This method provides a robust estimate of the trend slope and is less sensitive to outliers than some alternatives (e.g. linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = amax['date'].values.reshape(-1, 1)\n",
    "precipitation = amax['precipitation'].values\n",
    "\n",
    "model = TheilSenRegressor()\n",
    "model.fit(years, precipitation)\n",
    "\n",
    "trend_line = model.predict(years)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(amax['date'], amax['precipitation'], color='b', alpha=0.7, label='Data')\n",
    "plt.plot(years, trend_line, color='red', label='Trend Line (Theil-Sen)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Trend in Annual Max Daily Precipitation')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  How can we summarise the trend, and compare it with other sites? We can extract the slope as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = model.coef_[0]\n",
    "print(f\"Slope: {slope}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Taking it further\n",
    "\n",
    " Using the code above as a starting point, compute the slope of the trend for multiple basins.\n",
    "\n",
    "\n",
    "\n",
    " Have a look at Prosdocimi et al. ([NHESS](https://doi.org/10.5194/nhess-14-1125-2014), 2014) for inspiration.\n",
    "\n",
    "\n",
    "\n",
    " Think about the limitations of this analysis.\n",
    "\n",
    " - Where has the data come from?\n",
    "\n",
    " - Is a linear trend appropriate?\n",
    "\n",
    " - Is the annual maximum precipitation the best value to use? What else might be more appropriate for different applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## UK precipitation and the North Atlantic Oscillation\n",
    "\n",
    " In the lecture we spoke about the role of large-scale climate oscillations on precipitation patterns worldwide. In the UK, the North Atlantic Oscillation (NAO) exerts considerable influence on winter (December-February; DJF) precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First of all we are going to download the NAO index from the internet. The following code downloads the data using Python's `reqeuests` library and converts the data to a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from io import StringIO\n",
    "\n",
    "response = requests.get('https://crudata.uea.ac.uk/cru/data/nao/nao.dat')\n",
    "data = response.text\n",
    "\n",
    "# Convert string data into a StringIO object\n",
    "data_io = StringIO(data)\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "# Assuming the first value in each row is the index and the rest are data columns\n",
    "nao_data = pd.read_csv(data_io, sep='\\\\s+', header=None)\n",
    "\n",
    "# Set column names if needed\n",
    "nao_data.columns = ['year'] + [f'{i}' for i in range(1, 13)] + ['annual_mean']\n",
    "nao_data = nao_data.drop('annual_mean', axis=1)\n",
    "nao_data = nao_data.melt(id_vars=['year'], var_name='month', value_name='nao')\n",
    "nao_data.loc[nao_data['nao'] == -99.99, 'nao'] = np.nan\n",
    "\n",
    "# Convert the Year/month columns to a date, and make this the index\n",
    "nao_data['date'] = pd.to_datetime(nao_data['year'].astype(str) + '-' + nao_data['month'], format='%Y-%m')\n",
    "nao_data = nao_data.set_index('date')\n",
    "\n",
    "print(nao_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The resulting dataframe contains monthly NAO values from 1821 to the present year, with missing values represented as NaN. Now we want to compute the DJF seasonal average, discarding other months. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increment the year in December by 1, so that season_year is the year of the end month (i.e. Feb)\n",
    "nao_data['season_year'] = nao_data.index.year \n",
    "nao_data.loc[nao_data.index.month == 12, 'season_year'] += 1 \n",
    "\n",
    "# Define a custom season assignment\n",
    "season_mapping = {12: 'DJF', 1: 'DJF', 2: 'DJF'} \n",
    "\n",
    "# Add a 'season' column\n",
    "nao_data['season'] = nao_data.index.month.map(season_mapping)\n",
    "\n",
    "# Drop rows that don't belong to DJF:\n",
    "nao_data = nao_data[nao_data['season'] == 'DJF']\n",
    "\n",
    "# Finally, compute the seasonal averages:\n",
    "nao_data = nao_data.groupby(['season_year', 'season'])[['nao']].mean().reset_index()\n",
    "print(nao_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we need to prepare our rainfall data. Lets create a function to compute the DJF mean precipitation for any given catchment, using the same aggregation method as we used for the NAO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_djf_precip(x): \n",
    "\n",
    "    x['date'] = pd.to_datetime(x['date'])\n",
    "    x = x.set_index('date')\n",
    "    x['season_year'] = x.index.year \n",
    "    x.loc[x.index.month == 12, 'season_year'] += 1\n",
    "\n",
    "    # Define a custom season assignment\n",
    "    season_mapping = {12: 'DJF', 1: 'DJF', 2: 'DJF'} \n",
    "\n",
    "    # Add a 'season' column\n",
    "    x['season'] = x.index.month.map(season_mapping)\n",
    "\n",
    "    # Drop rows that don't belong to a season\n",
    "    x = x[x['season'] == 'DJF']\n",
    "\n",
    "    # Compute seasonal averages\n",
    "    x = x.groupby(['season_year', 'season'])[['precipitation']].mean().reset_index()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are interested in the relationship between preciptiation and NAO. Let's compute the correlation coefficient (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm \n",
    "\n",
    "data = pd.read_csv(\n",
    "    os.path.join(DATADIR, 'timeseries', f'CAMELS_GB_hydromet_timeseries_{id}_19701001-20150930.csv'), \n",
    "    parse_dates=[0])\n",
    "\n",
    "data = compute_djf_precip(data)\n",
    "data = data.merge(nao_data, how='left', on=['season_year', 'season'])\n",
    "pearson_corr = pearsonr(data['precipitation'], data['nao'])\n",
    "result = pd.DataFrame.from_dict({'id': [f'{id}'], 'pearsonr': pearson_corr.statistic, 'pvalue': pearson_corr.pvalue})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As in the previous notebook, let's load the catchment metadata (here we load the land use attributes - but it could be anything because we just want to get all the catchment IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_lu = pd.read_csv(os.path.join(DATADIR, f'CAMELS_GB_landcover_attributes.csv')) \n",
    "catchment_ids = metadata_lu['gauge_id'].to_list()\n",
    "print(len(catchment_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " OK, now we can loop through the catchments and compute the correlation of winter precipitation and the NAO index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we should have a dataframe with columns `id`, `spearmanr` and `pvalue`. We're interested in (i) the strength of the relationship in various catchments, and (ii) how the spatial pattern varies in space. How might we visualize this information? One nice approach would be to create a choropleth map. You can do this by joining the dataset you have just created with the catchment polygons in the CAMELS-GB dataset - `data/8344e4f3-d2ea-44f5-8afa-86d2987543a9/data/CAMELS_GB_catchment_boundaries.shp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
